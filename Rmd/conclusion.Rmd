## Conclusion

In our case study, which focuses on term deposit subscriptions, need to choose a model that *maximizes sensitivity without lowering precision too much*.

The **sensitivity** measures how many of the customers who would have subscribed were correctly identified by the model. The bank prefers to reach everyone who might potentially say “yes” even at the cost of contacting some who will eventually say “no” in order not to miss any possible customers.

On the other hand, we want to avoid decreasing **precision** too much. This metric measures, among all the instances the model predicts as “Yes”, how many actually said “Yes.” Maintaining high precision is important in our context, because contacting people who are not interested may harm the bank’s credibility and lead to wasted resources.

Therefore, we analyze the metrics of each previously selected model and choose the best one:

```{r, echo=FALSE}
final_table <- data.frame(
  Model = c('LASSO Logistic', 'Random Forest'),
  Threshold = c(0.20, best_threshold),
  Accuracy = c(res_lasso_02$Accuracy, round(accuracy, 3)),
  Precision = c(res_lasso_02$Precision, round(precision, 3)),
  Sensitivity = c(res_lasso_02$Sensitivity, round(sensitivity, 3)),
  Specificity = c(res_lasso_02$Specificity, round(specificity, 3)),
  F1_Score = c(res_lasso_02$F1, round(f1_score, 3))
)

knitr::kable(final_table, align = 'cccccc', caption = 'Final Model Comparison') %>%
  kableExtra::kable_styling(full_width = TRUE, position='center', bootstrap_options = c("striped"))
```

Overall, **Random Forest** with a threshold of `r round(best_threshold, 2)` appears to be the better-performing model. It achieves higher accuracy, precision, and F1 score, and it maintains strong specificity, offering a solid trade-off between detecting potential subscribers and avoiding unnecessary contacts.
