
```{r, echo=FALSE}
source("../R/MS_functions.R")
```

## Model selection
In this section, we explore different classification models to predict whether a client will subscribe to a term deposit.

Before training the models, we applied a transformation algorithm to convert categorical variables into numerical format. This is a crucial step in the data preprocessing phase, as many machine learning algorithms require numerical input. We used **one-hot encoding** to make categorical variables compatible with the classification models. This method represents **each category as a binary variable**, avoiding the introduction of arbitrary numerical orderings among categories. In this way, we ensure a correct statistical interpretation of qualitative variables and improve the effectiveness of the model training process.

However, among the classification models considered, LDA (Linear Discriminant Analysis) and QDA (Quadratic Discriminant Analysis) are not suitable in our case due to the nature of the predictor variables.
In particular, most of the independent variables are binary and do not satisfy the fundamental assumption of normal distribution within each class, which is required by both methods. Furthermore, both models rely on covariance structures, whose interpretation becomes limited when applied to dichotomous variables.

For these reasons, we decided not to pursue further analysis with LDA and QDA and instead focused on models that are better aligned with the structure of the data, such as **Logistic Regression** and **Random Forest**.

### Preprocessing
Based on the Exploratory Data Analysis (EDA), we selected only the most relevant variables.

With a view to training the model, we apply **one-hot encoding**. We obtain the following dataset:

```{r, echo=FALSE}
train_bin <- train

train_bin$cons.price.idx <- ifelse(train_bin$cons.price.idx < 93, 1, 0)
names(train_bin)[names(train_bin) == "cons.price.idx"] <- "low_cpi"

train_bin$cons.conf.idx <- ifelse(train_bin$cons.conf.idx > median(train$cons.conf.idx), 1, 0)
names(train_bin)[names(train_bin) == "cons.conf.idx"] <- "high_cci"

train_bin$euribor3m <- ifelse(train_bin$euribor3m < mean(train$euribor3m), 1, 0)
names(train_bin)[names(train_bin) == "euribor3m"] <- "low_euribor"

train_bin$emp.var.rate <- ifelse(train_bin$emp.var.rate < 0, 1, 0)
names(train_bin)[names(train_bin) == "emp.var.rate"] <- "negative_emp"

train_bin$university<-ifelse(train_bin$education=='university.degree', 1, 0)
train_bin$p_course<-ifelse(train_bin$education=='professional.course', 1, 0)
train_bin <- subset(train_bin, select = -education)

train_bin$job_student <- ifelse(train_bin$job == "student", 1, 0)
train_bin$job_retired <- ifelse(train_bin$job == "retired", 1, 0)
train_bin$job_admin <- ifelse(train_bin$job == "admin.", 1, 0)
train_bin <- subset(train_bin, select = -job)

train_bin$month_sep <- ifelse(train_bin$month == "sep", 1, 0)
train_bin$month_oct <- ifelse(train_bin$month == "oct", 1, 0)
train_bin$month_dec <- ifelse(train_bin$month == "dec", 1, 0)
train_bin$month_mar <- ifelse(train_bin$month == "mar", 1, 0)

train_bin$p_failure <- ifelse(train_bin$poutcome == "failure", 1, 0)
train_bin$p_success <- ifelse(train_bin$poutcome == "success", 1, 0)
train_bin <- subset(train_bin, select = -poutcome)

train_bin$contact <- ifelse(train_bin$contact == "cellular", 1, 0)
names(train_bin)[names(train_bin) == "contact"] <- "cellular"

train_bin$marital <- ifelse(train_bin$marital == "single", 1, 0)
names(train_bin)[names(train_bin) == "marital"] <- "single"

train_bin$campaign <- ifelse(train_bin$campaign > 5, 0, 1)
names(train_bin)[names(train_bin) == "campaign"] <- "low_call"

```

```{r, echo=FALSE}
drop_var<-c('pdays', "default", 'housing' ,"loan", "day_of_week", 
            "month", "nr.employed", 'job', "duration", "emp_cat")

full_df <- train_bin[, !(names(train_bin) %in% drop_var)]
full_df$target <-factor(ifelse(subscribed=='yes', 1, 0))
full_df<-full_df[, !(names(full_df)) %in% 'subscribed']
detach(train)
attach(full_df)
```

```{r, echo=FALSE}
varDf <- data.frame(
  Variable = c("age", "single", "cellular", "low_call", "previous", "negative_emp", "low_cpi", "high_cci", "low_euribor", "university", "p_course", "job_student", "job_retired", "job_admin", "month_sep", "month_oct", "month_dec", "month_mar", "p_failure", "p_success", "target"),
  Type = c("int", "bool", "bool", "bool", "int", "bool", "bool", "bool", "bool", "bool", "bool", "bool", "bool", "bool", "bool", "bool", "bool", "bool", "bool", "bool", "bool")
)

knitr::kable(varDf, align = "ll", caption = "21 Variable") %>%
kableExtra::kable_styling(full_width = TRUE, position = "center", bootstrap_options = "striped")
```

### Logistic Regression

**Logistic regression** is a widely used statistical model for binary **classification** tasks. It is based on the sigmoid (logistic) function, which maps any real-valued input into the interval (0, 1), making the output interpretable as a **probability**. Specifically, the probability that an observation belongs to class 1 is given by:

<div class="center">
$P(Y=1|X=x)=p(x)=\frac{e^{\beta_0+\beta_1x_1+\dots + \beta_n  x_n}}{1+e^{\beta_0+\beta_1x_1+\dots + \beta_n x_n}}$
</div>

To perform classification, a **decision threshold** is applied: if $p(x)$ exceeds the threshold (commonly 0.5), the observation is assigned to class 1; otherwise, it is assigned to class 0.

Before training the model on the training set, we applied **two variable selection methods** to the entire dataset in order to reduce the large number of predictors. The selection criteria were: 

- **Stepwise** selection in both directions, that optimizes the AIC at each step.

- **LASSO,** which, as supported by the literature, often outperforms stepwise methods. 

After that we checked for VIF, but for both models **there wasnâ€™t any multicollinearity.**

Once the relevant variables were selected using both methods, we employed **k-fold cross-validation** to obtain stable estimates of accuracy, misclassification error, and other useful metrics which will be used for comparison. We chose 10 folds, as it offers a good balance between computational efficiency, compared to LOOCV, and reliable model evaluation, as compared to 5-fold cross-validation.

The variables we got from stepwise and LASSO are the following ones:

```{r, echo= FALSE}
df <- data.frame(
  Full_dataset = c('age', 'single', 'cellular', 'low_call', 'previous', 'negative_emp', 'low_cpi', 'high_cci', 'low_euribor', 'university', 'p_course', 'job_student', 'job_retired', 'job_admin', 'month_sep', 'month_oct', 'month_dec', 'month_mar', 'p_failure', 'p_success', 'total'),
  Stepwise_Model = c(' ', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', '20'),
  Lasso_Model=c(' ', ' ', 'x', ' ', ' ', 'x', 'x', 'x', 'x', 'x', ' ', 'x', 'x', ' ', 'x', 'x', 'x', 'x', 'x', 'x', '14')
)

knitr::kable(df, align = 'ccc', caption = 'Model Variable Comparison')%>%
  kableExtra::kable_styling(full_width = TRUE, position='center', bootstrap_options = c("striped"))

```

As we can see, **LASSO** was able to **shrink the number of variables more than Stepwise**. It's a simpler model. Let's now compare the two models at different thresholds. The **thresholds** maximize: 

- **Accuracy,** that takes into account just the general accuracy of the model.

- **F1** which is a metric useful in unbalanced dataset like ours, since it considers both precision (how many predicted positives are actual positives) and recall (how many actual positives are detected).

We also have computed Sensitivity and Specificity.

```{r echo=FALSE, message=FALSE, warning=FALSE}
full_model <- glm(target ~ ., data = full_df, family = binomial)
stepwise <- stepAIC(full_model, direction = "both", trace = FALSE)

# predictore removed by Stepwise
stepwise_formula <- target ~ single + cellular + low_call + previous + negative_emp + 
  low_cpi + high_cci + low_euribor + university + p_course + 
  job_student + job_retired + job_admin + month_sep + month_oct + 
  month_dec + month_mar + p_failure + p_success



set.seed(123)

df_no_target <- subset(full_df, select = -target)

fit_lasso <- glmnet(x = as.matrix(df_no_target),
                    y = target,
                    alpha = 1,
                    family = "binomial",
)

cv_fit <- cv.glmnet(
  x = as.matrix(df_no_target),
  y = target,
  alpha = 1,
  family = "binomial"
)

# predictors selected by Lasso

lasso_formula <- target ~ cellular + negative_emp + 
  low_cpi + high_cci + low_euribor + university + 
  job_student + job_retired + month_sep + month_oct + 
  month_dec + month_mar + p_failure + p_success

lasso_mod<-glm(lasso_formula, data=full_df, family=binomial)
```


```{r echo=FALSE, message=FALSE}
# Compare the models
stepwise_results <- k_fold_mod(data = full_df, target_col = "target", model_formula = stepwise)
lasso_results    <- k_fold_mod(data = full_df, target_col = "target", model_formula = lasso_mod)

# Threshold evaluation
probs_stepwise <- predict(stepwise, type = "response")
probs_lasso    <- predict(lasso_mod, type = "response")

res_step_05 <- evaluate_threshold(probs_stepwise, target, 0.5)
res_step_02 <- evaluate_threshold(probs_stepwise, target, 0.2)

res_lasso_05 <- evaluate_threshold(probs_lasso, target, 0.5)
res_lasso_02 <- evaluate_threshold(probs_lasso, target, 0.2)


# Unisci tutti i risultati in una lista
results_list <- list(
  Stepwise_0.5 = res_step_05,
  Stepwise_0.2 = res_step_02,
  LASSO_0.5    = res_lasso_05,
  LASSO_0.2    = res_lasso_02
)

# Trasforma in data.frame
results_df <- do.call(rbind, lapply(names(results_list), function(name) {
  res <- results_list[[name]]
  model <- sub("_.*", "", name)
  threshold <- res$Threshold
  data.frame(
    Model       = model,
    Threshold   = threshold,
    Accuracy    = res$Accuracy,
    F1          = res$F1,
    Sensitivity = res$Sensitivity,
    Specificity = res$Specificity
  )
}))

# Visualizza il risultato

knitr::kable(results_df, align = 'ccccc', caption = 'Model Performance at Different Thresholds')  %>%
  kableExtra::kable_styling(full_width = TRUE, position='center', bootstrap_options = c("striped"))

```
The table shows that both models perform similarly in terms of accuracy and specificity. However, when the threshold is lowered from 0.5 to 0.2, both models achieve higher F1 scores and sensitivity, though at the expense of reduced accuracy and specificity.

Between the two models, the LASSO-based logistic regression is slightly preferable, as it achieves comparable or slightly better F1 scores while benefiting from variable selection and model simplicity.

Given the imbalance in the dataset, we prioritize a threshold that maximizes the F1 score rather than overall accuracy. This is because both false positives (wasting resources contacting uninterested customers) and false negatives (missing likely subscribers) are costly in this context. Therefore, a threshold that better balances precision and recall, reflected by a higher F1 score, is more appropriate for the bankâ€™s decision-making.

### Decision tree

```{r echo=FALSE}
df_for_Tree <- full_df
df_for_Tree$target <- factor(ifelse(full_df$target == 1, "Yes", "No"))

train_index <- createDataPartition(df_for_Tree$target, p = 0.8, list = FALSE)

train_set <- df_for_Tree[train_index, ]
test_set  <- df_for_Tree[-train_index, ]
```

```{r echo=FALSE}
set.seed(123)
treeDf <- tree(target ~ ., train_set)

#summary(treeDf)
```

```{r echo=FALSE}
plot(treeDf)
text(treeDf, pretty = 0)
```

#### Random Forest

```{r}
set.seed(123)
rf_train <- randomForest(target ~ ., train_set, mtry = 7, importance = TRUE)
rf_train
```

```{r}
tree_pred <- predict(rf_train, newdata = test_set)
table(tree_pred, test_set$target)
```

```{r echo=FALSE}
#importance(rf_train)
```

```{r echo=FALSE}
varImpPlot(rf_train)
```

#### Boosting

```{r echo=FALSE}
train_set2 <- full_df[train_index, ]
train_set2$target <- as.numeric(as.character(train_set2$target))

test_set2 <- full_df[-train_index, ]
test_set2$target <- as.numeric(as.character(test_set2$target))
```

```{r}
set.seed(123)
boost_train <- gbm(target ~ ., data = train_set2, distribution = "bernoulli", n.trees = 5000, interaction.depth = 3)
```


```{r echo=FALSE}
summary(boost_train)
```


```{r}
yhat.boost <- predict(boost_train, newdata = test_set2, n.trees = 5000)
table(pred = yhat.boost > 0.5, actual=test_set2$target)
```
