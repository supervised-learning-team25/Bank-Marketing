
```{r, echo=FALSE}
source("../R/MS_functions.R")
```

## Model selection
In this section, we explore different classification models to predict whether a client will subscribe to a term deposit.

Before training the models, we applied a transformation algorithm to convert categorical variables into numerical format. This is a crucial step in the data preprocessing phase, as many machine learning algorithms require numerical input. We used **one-hot encoding** to make categorical variables compatible with the classification models. This method represents **each category as a binary variable**, avoiding the introduction of arbitrary numerical orderings among categories. In this way, we ensure a correct statistical interpretation of qualitative variables and improve the effectiveness of the model training process.

However, among the classification models considered, LDA (Linear Discriminant Analysis) and QDA (Quadratic Discriminant Analysis) are not suitable in our case due to the nature of the predictor variables.
In particular, most of the independent variables are binary and do not satisfy the fundamental assumption of normal distribution within each class, which is required by both methods. Furthermore, both models rely on covariance structures, whose interpretation becomes limited when applied to dichotomous variables.

For these reasons, we decided not to pursue further analysis with LDA and QDA and instead focused on models that are better aligned with the structure of the data, such as **Logistic Regression**, **Random Forest** and **Boosting**.

### Preprocessing
Based on the Exploratory Data Analysis (EDA), we selected only the most relevant variables.

With a view to training the model, we apply **one-hot encoding**. We obtain the following dataset:

```{r, echo=FALSE}
train_bin <- train

train_bin$cons.price.idx <- ifelse(train_bin$cons.price.idx < 93, 1, 0)
names(train_bin)[names(train_bin) == "cons.price.idx"] <- "low_cpi"

train_bin$cons.conf.idx <- ifelse(train_bin$cons.conf.idx > median(train$cons.conf.idx), 1, 0)
names(train_bin)[names(train_bin) == "cons.conf.idx"] <- "high_cci"

train_bin$euribor3m <- ifelse(train_bin$euribor3m < mean(train$euribor3m), 1, 0)
names(train_bin)[names(train_bin) == "euribor3m"] <- "low_euribor"

train_bin$emp.var.rate <- ifelse(train_bin$emp.var.rate < 0, 1, 0)
names(train_bin)[names(train_bin) == "emp.var.rate"] <- "negative_emp"

train_bin$university<-ifelse(train_bin$education=='university.degree', 1, 0)
train_bin$p_course<-ifelse(train_bin$education=='professional.course', 1, 0)
train_bin <- subset(train_bin, select = -education)

train_bin$job_student <- ifelse(train_bin$job == "student", 1, 0)
train_bin$job_retired <- ifelse(train_bin$job == "retired", 1, 0)
train_bin$job_admin <- ifelse(train_bin$job == "admin.", 1, 0)
train_bin <- subset(train_bin, select = -job)

train_bin$month_sep <- ifelse(train_bin$month == "sep", 1, 0)
train_bin$month_oct <- ifelse(train_bin$month == "oct", 1, 0)
train_bin$month_dec <- ifelse(train_bin$month == "dec", 1, 0)
train_bin$month_mar <- ifelse(train_bin$month == "mar", 1, 0)

train_bin$p_failure <- ifelse(train_bin$poutcome == "failure", 1, 0)
train_bin$p_success <- ifelse(train_bin$poutcome == "success", 1, 0)
train_bin <- subset(train_bin, select = -poutcome)

train_bin$contact <- ifelse(train_bin$contact == "cellular", 1, 0)
names(train_bin)[names(train_bin) == "contact"] <- "cellular"

train_bin$marital <- ifelse(train_bin$marital == "single", 1, 0)
names(train_bin)[names(train_bin) == "marital"] <- "single"

train_bin$campaign <- ifelse(train_bin$campaign > 5, 0, 1)
names(train_bin)[names(train_bin) == "campaign"] <- "low_call"

```

```{r, echo=FALSE}
drop_var<-c('pdays', "default", 'housing' ,"loan", "day_of_week", 
            "month", "nr.employed", 'job', "duration", "emp_cat")

full_df <- train_bin[, !(names(train_bin) %in% drop_var)]
full_df$target <-factor(ifelse(subscribed=='yes', 1, 0))
full_df<-full_df[, !(names(full_df)) %in% 'subscribed']
detach(train)
attach(full_df)
```

```{r, echo=FALSE}
varDf <- data.frame(
  Variable = c("age", "single", "cellular", "low_call", "previous", "negative_emp", "low_cpi", "high_cci", "low_euribor", "university", "p_course", "job_student", "job_retired", "job_admin", "month_sep", "month_oct", "month_dec", "month_mar", "p_failure", "p_success", "target"),
  Type = c("int", "bool", "bool", "bool", "int", "bool", "bool", "bool", "bool", "bool", "bool", "bool", "bool", "bool", "bool", "bool", "bool", "bool", "bool", "bool", "bool")
)

knitr::kable(varDf, align = "ll", caption = "Selected Variables (21)") %>%
kableExtra::kable_styling(full_width = TRUE, position = "center", bootstrap_options = "striped")
```

### Logistic Regression

**Logistic regression** is a widely used statistical model for binary **classification** tasks. It is based on the sigmoid (logistic) function, which maps any real-valued input into the interval (0, 1), making the output interpretable as a **probability**. Specifically, the probability that an observation belongs to class 1 is given by:

<div class="center">
$P(Y=1|X=x)=p(x)=\frac{e^{\beta_0+\beta_1x_1+\dots + \beta_n  x_n}}{1+e^{\beta_0+\beta_1x_1+\dots + \beta_n x_n}}$
</div>

To perform classification, a **decision threshold** is applied: if $p(x)$ exceeds the threshold (commonly 0.5), the observation is assigned to class 1; otherwise, it is assigned to class 0.

Before training the model on the training set, we applied **two variable selection methods** to the entire dataset in order to reduce the large number of predictors. The selection criteria were: 

- **Stepwise** selection in both directions, that optimizes the AIC at each step.

- **LASSO,** which, as supported by the literature, often outperforms stepwise methods. 

After that we checked for VIF, but for both models **there wasn’t any multicollinearity.**

Once the relevant variables were selected using both methods, we employed **k-fold cross-validation** to obtain stable estimates of accuracy, misclassification error, and other useful metrics which will be used for comparison. We chose 10 folds, as it offers a good balance between computational efficiency, compared to LOOCV, and reliable model evaluation, as compared to 5-fold cross-validation.

The variables we got from stepwise and LASSO are the following ones:

```{r, echo= FALSE}
df <- data.frame(
  Full_dataset = c('age', 'single', 'cellular', 'low_call', 'previous', 'negative_emp', 'low_cpi', 'high_cci', 'low_euribor', 'university', 'p_course', 'job_student', 'job_retired', 'job_admin', 'month_sep', 'month_oct', 'month_dec', 'month_mar', 'p_failure', 'p_success', 'total'),
  Stepwise_Model = c(' ', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', '20'),
  Lasso_Model=c(' ', ' ', 'x', ' ', ' ', 'x', 'x', 'x', 'x', 'x', ' ', 'x', 'x', ' ', 'x', 'x', 'x', 'x', 'x', 'x', '14')
)

knitr::kable(df, align = 'ccc', caption = 'Model Variable Comparison')%>%
  kableExtra::kable_styling(full_width = TRUE, position='center', bootstrap_options = c("striped"))

```

As we can see, **LASSO** was able to **shrink the number of variables more than Stepwise**. It's a simpler model. Let's now compare the two models at different thresholds. The **thresholds** maximize: 

- **Accuracy,** that takes into account just the general accuracy of the model.

- **F1** which is a metric useful in unbalanced dataset like ours, since it considers both precision (how many predicted positives are actual positives) and recall, or Sensitivity, (how many actual positives are detected).

We also have computed Specificity (how many actual negatives are detected).

```{r echo=FALSE, message=FALSE, warning=FALSE}
full_model <- glm(target ~ ., data = full_df, family = binomial)
stepwise <- stepAIC(full_model, direction = "both", trace = FALSE)

# predictore removed by Stepwise
stepwise_formula <- target ~ single + cellular + low_call + previous + negative_emp + 
  low_cpi + high_cci + low_euribor + university + p_course + 
  job_student + job_retired + job_admin + month_sep + month_oct + 
  month_dec + month_mar + p_failure + p_success



set.seed(123)

df_no_target <- subset(full_df, select = -target)

fit_lasso <- glmnet(x = as.matrix(df_no_target),
                    y = target,
                    alpha = 1,
                    family = "binomial",
)

cv_fit <- cv.glmnet(
  x = as.matrix(df_no_target),
  y = target,
  alpha = 1,
  family = "binomial"
)

# predictors selected by Lasso

lasso_formula <- target ~ cellular + negative_emp + 
  low_cpi + high_cci + low_euribor + university + 
  job_student + job_retired + month_sep + month_oct + 
  month_dec + month_mar + p_failure + p_success

lasso_mod<-glm(lasso_formula, data=full_df, family=binomial)
```


```{r echo=FALSE, message=FALSE}
# Compare the models
stepwise_results <- k_fold_mod(data = full_df, target_col = "target", model_formula = stepwise)
lasso_results    <- k_fold_mod(data = full_df, target_col = "target", model_formula = lasso_mod)

# Threshold evaluation
probs_stepwise <- predict(stepwise, type = "response")
probs_lasso    <- predict(lasso_mod, type = "response")

res_step_05 <- evaluate_threshold(probs_stepwise, target, 0.5)
res_step_02 <- evaluate_threshold(probs_stepwise, target, 0.2)

res_lasso_05 <- evaluate_threshold(probs_lasso, target, 0.5)
res_lasso_02 <- evaluate_threshold(probs_lasso, target, 0.2)


# Unisci tutti i risultati in una lista
results_list <- list(
  Stepwise_0.5 = res_step_05,
  Stepwise_0.2 = res_step_02,
  LASSO_0.5    = res_lasso_05,
  LASSO_0.2    = res_lasso_02
)

# Trasforma in data.frame
results_df <- do.call(rbind, lapply(names(results_list), function(name) {
  data.frame(Model_Threshold = name, as.data.frame(t(unlist(results_list[[name]]))))
}))

# Visualizza il risultato

knitr::kable(results_df, align = 'ccccc', caption = 'Model Performance at Different Thresholds')  %>%
  kableExtra::kable_styling(full_width = TRUE, position='center', bootstrap_options = c("striped"))

```
The table shows that both models perform similarly in terms of accuracy and specificity. However, when the threshold is lowered from 0.5 to 0.2, both models achieve higher F1 scores and sensitivity, though at the expense of reduced accuracy and specificity.

Between the two models, the LASSO-based logistic regression is slightly preferable, as it achieves comparable or slightly better F1 scores while benefiting from variable selection and model simplicity.

Given the imbalance in the dataset, we prioritize a threshold that maximizes the F1 score rather than overall accuracy. This is because both false positives (wasting resources contacting uninterested customers, and potentially harming the bank reputation) and false negatives (missing likely subscribers) are costly in this context. Therefore, a threshold that better balances precision and recall (Sensitivity), reflected by a higher F1 score, is more appropriate for the bank’s decision-making.

### Decision Tree Ensemble Models

Now we explore decision tree ensemble methods, which combine multiple decision trees to improve predictive performance and robustness. Unlike a single decision tree, which may suffer from high variance or overfitting, ensemble techniques like **Random Forest** and **Boosting** aggregate the predictions of many trees to achieve better generalization. In the following sections, we will train and evaluate models using both approaches, starting with Random Forest.

```{r echo=FALSE}
df_for_Tree <- full_df
df_for_Tree$target <- factor(ifelse(full_df$target == 1, "Yes", "No"))

set.seed(123)
train_index <- createDataPartition(df_for_Tree$target, p = 0.8, list = FALSE)

train_set <- df_for_Tree[train_index, ]
test_set  <- df_for_Tree[-train_index, ]
```

#### Random Forest

Let us train a **Random Forest classifier**. We set the mtry = $\sqrt{n}$, where n is the number of variable, to control the number of variables randomly selected at each split. The model was trained on the full training set with 500 trees and feature importance enabled.

```{r echo=FALSE}
set.seed(123)
rf_model <- randomForest(target ~ ., train_set, mtry = 4, importance = TRUE)
```

The model return an **out-of-bag (OOB) error rate** of `r round(tail(rf_model$err.rate[, "OOB"], 1) * 100, 2)`%, it's a good level of error, indicates a fairly robust generalization without overfitting.

```{r echo=FALSE}

# Vediamo alla fine le metriche
#tree_pred <- predict(rf_train, newdata = test_set)
#table(tree_pred, test_set$target)
```

The following graphs represent the importance of each variable for accuracy and Gini impurity.

```{r echo=FALSE}
varImpPlot(rf_model)
```

Both measures highlight similar variables as important, suggesting consistent influence of features such as `age`, `p_success`, and `high_cci`. However, there are slight differences in ranking due to how each metric evaluates importance.

#### Boosting

Let us train a **Boosting classifier**, setting the number of trees to 5000 and the maximum depth of each tree to 3.

```{r echo=FALSE}
train_set2 <- full_df[train_index, ]
train_set2$target <- as.numeric(as.character(train_set2$target))

test_set2 <- full_df[-train_index, ]
test_set2$target <- as.numeric(as.character(test_set2$target))
```

```{r include=FALSE}
set.seed(123)
boost_train <- gbm(target ~ ., data = train_set2, distribution = "bernoulli", n.trees = 5000, interaction.depth = 3)
tmp <- summary(boost_train)
```

The following graph represent the importance of each variable.

<div class="hscroll-plot" style="height: 450px;">

<div class='plot-container'>
```{r echo=FALSE}
tmp$var <- factor(tmp$var, levels = tmp$var[order(tmp$rel.inf)])

ggplot(tmp, aes(x = var, y = rel.inf)) +
  geom_bar(stat = "identity", fill = "steelblue", alpha = 0.9) +
  coord_flip() +
  labs(title = "Relative Importance of Variables",
       x = NULL,
       y = "Relative Importance") +
  theme_minimal()
```
</div>

<div class='plot-container'>
```{r echo=FALSE}
# log-odds
partial <- plot(boost_train, i = "age", return.grid = TRUE)

# convert log-odds to probability
partial$prob <- 1 / (1 + exp(-partial$y))

# plot
ggplot(partial, aes(x = age, y = prob)) +
  geom_line(color = "steelblue", size = 1) +
  geom_hline(yintercept = 0.11, linetype = "dashed", color = "red") +
  labs(
    title = "Effect of 'AGE' on the predicted probability",
    x = "Age",
    y = "Probability of subscribing term deposit",
  ) +
  theme_minimal()
```
</div>
</div>

<div class="plot-text">
For the boosting model, the most important variables are similar to those identified by the random forest. However, in boosting, `previous` appears to be more relevant.
</div>

#### Random forest vs Boosting

As previously discussed, we calculate, for each model, the threshold that maximizes the F1 score, since this metric provides a balanced measure of precision and recall. This approach is particularly suitable in scenarios with class imbalance, where both false positives and false negatives are critical.

```{r echo=FALSE}
# Random Forest

probs_rf <- predict(rf_model, newdata = test_set, type = "prob")[, "Yes"]

# Founrd the best threshold
best_f1 <- 0
best_threshold <- 0
thresholds <- seq(0.01, 0.99, by = 0.01)

for (t in thresholds) {
  preds <- ifelse(probs_rf >= t, "Yes", "No")
  f1 <- F1_Score(y_pred = preds, y_true = test_set$target, positive = "Yes")
  
  if (!is.na(f1) && f1 > best_f1) {
    best_f1 <- f1
    best_threshold <- t
  }
}
```

```{r echo=FALSE}
tree_pred_thresh <- ifelse(probs_rf >= best_threshold, "Yes", "No")

# accuracy
accuracy <- sum(tree_pred_thresh == test_set$target) / nrow(test_set)
#precision
precision <- sum(tree_pred_thresh == "Yes" & test_set$target == "Yes") / sum(tree_pred_thresh == "Yes")
# specificity
specificity <- sum(tree_pred_thresh == "No" & test_set$target == "No") / sum(test_set$target == "No")
# sensitivity
sensitivity <- sum(tree_pred_thresh == "Yes" & test_set$target == "Yes") / sum(test_set$target == "Yes")
# F1 score
f1_score <- F1_Score(y_pred = tree_pred_thresh, y_true = test_set$target, positive = "Yes")
```

```{r echo=FALSE}
# Random Forest con threshold 0.5
tree_pred_05 <- ifelse(probs_rf >= 0.5, "Yes", "No")

accuracy_05 <- mean(tree_pred_05 == test_set$target)
precision_o5 <- sum(tree_pred_05 == "Yes" & test_set$target == "Yes") / sum(tree_pred_05 == "Yes")
specificity_05 <- sum(tree_pred_05 == "No" & test_set$target == "No") / sum(test_set$target == "No")
sensitivity_05 <- sum(tree_pred_05 == "Yes" & test_set$target == "Yes") / sum(test_set$target == "Yes")
f1_score_05 <- F1_Score(y_pred = tree_pred_05, y_true = test_set$target, positive = "Yes")
```

```{r echo=FALSE, message=FALSE}
probs_bo <- predict(boost_train, newdata = test_set2, n.trees = 5000, type = "response")

roc_obj <- roc(test_set2$target, probs_bo)
best_thresh <- coords(roc_obj, x = "best", best.method = "youden")

y_true <- test_set2$target
y_pred <- ifelse(probs_bo > best_thresh$threshold, 1, 0)

conf_matrix <- table(Predicted = y_pred, True = y_true)

# metric
accuracy_bo <- mean(y_pred == y_true)
precision_bo <- sum(y_pred == 1 & y_true == 1) / sum(y_pred == 1)
specificity_bo <- sum(y_pred == 0 & y_true == 0) / sum(y_true == 0)
sensitivity_bo <- sum(y_pred == 1 & y_true == 1) / sum(y_true == 1)
f1_score_bo <- F1_Score(y_pred = y_pred, y_true = y_true, positive = "1")
```

```{r echo=FALSE}
# Boosting con threshold 0.5
y_pred_bo_05 <- ifelse(probs_bo > 0.5, 1, 0)

accuracy_bo_05 <- mean(y_pred_bo_05 == y_true)
precision_bo_05 <- sum(y_pred_bo_05 == 1 & y_true == 1) / sum(y_pred_bo_05 == 1)
specificity_bo_05 <- sum(y_pred_bo_05 == 0 & y_true == 0) / sum(y_true == 0)
sensitivity_bo_05 <- sum(y_pred_bo_05 == 1 & y_true == 1) / sum(y_true == 1)
f1_score_bo_05 <- F1_Score(y_pred = y_pred_bo_05, y_true = y_true, positive = "1")
```

```{r echo=FALSE}
#results_tree <- data.frame(
#  Model = c("Random Forest", "Boosting"),
#  Threshold = c(0.12, 0.15),
#  Accuracy = c(round(accuracy, 3), round(accuracy_bo, 3)),
#  Specificity = c(round(specificity, 3), round(specificity_bo, 3)),
#  Sensitivity = c(round(sensitivity, 3), round(sensitivity_bo, 3)),
#  F1_Score = c(round(f1_score, 3), round(f1_score_bo,3)))

results_tree <- data.frame(
  Model = c("Random Forest 0.5", 
            "Random Forest 0.03", 
            "Boosting 0.5", 
            "Boosting 0.11"),
  Threshold = c(0.5, 
                round(best_threshold, 2), 
                0.5, 
                round(best_thresh$threshold, 2)),
  Accuracy = c(round(accuracy_05, 3), 
               round(accuracy, 3), 
               round(accuracy_bo_05, 3), 
               round(accuracy_bo, 3)),
  Precision = c(round(precision_o5, 3), 
                  round(precision, 3), 
                  round(precision_bo_05, 3), 
                  round(precision_bo, 3)),
  Sensitivity = c(round(sensitivity_05, 3), 
                  round(sensitivity, 3), 
                  round(sensitivity_bo_05, 3), 
                  round(sensitivity_bo, 3)),
  Specificity = c(round(specificity_05, 3), 
                  round(specificity, 3), 
                  round(specificity_bo_05, 3), 
                  round(specificity_bo, 3)),
  F1_Score = c(round(f1_score_05, 3), 
               round(f1_score, 3), 
               round(f1_score_bo_05, 3), 
               round(f1_score_bo, 3))
)
```

In our case, the optimal threshold was found to be `r round(best_threshold, 2)` for Random Forest and `r round(best_thresh$threshold, 2)` for Boosting. 

Using this values, we obtain the following performance metrics:

```{r echo=FALSE}
knitr::kable(results_tree, align = 'cccccc', caption = 'Random forest vs Boosting')%>%
  kableExtra::kable_styling(full_width = TRUE, position='center', bootstrap_options = c("striped"))
```

At the default threshold (0.50), Random Forest is similar to Boosting. Both models show high specificity (i.e., they are good at detecting the negative class), but very low sensitivity — indicating poor performance in identifying positive cases.

When using the threshold that maximizes the F1 score, both models improve their sensitivity at the cost of some specificity. The **Random Forest** model achieves the highest F1 score (`r round(f1_score, 3)`), providing the best balance between precision and recall. Although the **Boosting** model reaches a higher sensitivity (`r round(sensitivity_bo, 3)`), it sacrifices more accuracy and specificity, resulting in a lower F1 score overall.

Random Forest with a threshold of `r round(best_threshold, 2)` is the best-performing model overall, as it achieves the highest F1 score while maintaining a reasonable trade-off between all metrics. This suggests it offers the most balanced generalization ability for this classification task.
