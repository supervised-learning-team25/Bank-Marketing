## Model selection

### Preprocessing
With a view to training the model, we apply **one-hot encoding**.

Based on the Exploratory Data Analysis (EDA), we selected only the most relevant variables and according to the following patterns:

* People with a university degree or a professional training course are more likely to subscribe.
* Individuals who are students or retired show a higher tendency to subscribe.
* There are more subscriptions in the `months` of September, October, December, and March.
* People who in the `previous campaign` either subscribed or refused are more likely to subscribe, compared to those who were not contacted.
* `cons.price.idx` values greater than 93 are linked to a higher chance of subscription.
* `cons.conf.idx` values above the median are associated with a higher likelihood of subscription.
* `euribor3m` values below the mean correspond to a higher probability of subscription.
* `emp.var.rate` values below 0 are more likely to be associated with subscription.

We transform these continuous variables into binary indicators reflecting these insights. 
```{r echo=FALSE}
train_bin <- train

train_bin$cons.price.idx <- ifelse(train_bin$cons.price.idx < 93, 1, 0)
names(train_bin)[names(train_bin) == "cons.price.idx"] <- "low_cpi"

train_bin$cons.conf.idx <- ifelse(train_bin$cons.conf.idx > median(train$cons.conf.idx), 1, 0)
names(train_bin)[names(train_bin) == "cons.conf.idx"] <- "high_cci"

train_bin$euribor3m <- ifelse(train_bin$euribor3m < mean(train$euribor3m), 1, 0)
names(train_bin)[names(train_bin) == "euribor3m"] <- "low_euribor"

train_bin$emp.var.rate <- ifelse(train_bin$emp.var.rate < 0, 1, 0)
names(train_bin)[names(train_bin) == "emp.var.rate"] <- "negative_emp"

train_bin$university<-ifelse(train_bin$education=='university.degree', 1, 0)
train_bin$p_course<-ifelse(train_bin$education=='professional.course', 1, 0)
train_bin <- subset(train_bin, select = -education)

train_bin$job_student <- ifelse(train_bin$job == "student", 1, 0)
train_bin$job_retired <- ifelse(train_bin$job == "retired", 1, 0)
train_bin$job_admin <- ifelse(train_bin$job == "admin.", 1, 0)
train_bin <- subset(train_bin, select = -job)

train_bin$month_sep <- ifelse(train_bin$month == "sep", 1, 0)
train_bin$month_oct <- ifelse(train_bin$month == "oct", 1, 0)
train_bin$month_dec <- ifelse(train_bin$month == "dec", 1, 0)
train_bin$month_mar <- ifelse(train_bin$month == "mar", 1, 0)

train_bin$p_failure <- ifelse(train_bin$poutcome == "failure", 1, 0)
train_bin$p_success <- ifelse(train_bin$poutcome == "success", 1, 0)
train_bin <- subset(train_bin, select = -poutcome)
```

```{r echo=FALSE}
drop_var<-c("marital", "default", 'housing' ,"loan", 
            "contact","day_of_week", "month", "campaign",  "nr.employed", 'job', "duration", "pdays")

full_df <- train_bin[, !(names(train_bin) %in% drop_var)]
full_df$target <-ifelse(subscribed=='yes', 1, 0)
full_df<-full_df[, !(names(full_df)) %in% 'subscribed']
detach(train)
attach(full_df)
```

We obtain the following dataset:
```{r echo=FALSE}
str(full_df)
```

### STEPWISE selection

```{r}
full_model <- glm(target ~ ., data = full_df, family = binomial)
stepwise <- stepAIC(full_model, direction = "both", trace = FALSE)
vif(stepwise)

# predictore removed by Stepwise
stepwise$anova 
```

### LASSO selection
```{r}
df_no_target <- subset(full_df, select = -target)

fit_lasso <- glmnet(x = as.matrix(df_no_target),
                    y = target,
                    alpha = 1,
                    family = "binomial",
)

cv_fit <- cv.glmnet(
  x = as.matrix(df_no_target),
  y = target,
  alpha = 1,
  family = "binomial"
)

plot(cv_fit)

# predictors selected by Lasso
coef(cv_fit, s = "lambda.1se")
lasso_mod<-glm(target~negative_emp+low_cpi+high_cci+low_euribor+university+job_student+
                 job_retired+month_sep+month_oct+month_dec+month_mar+p_failure+p_success, data=full_df, family=binomial)
```

```{r}
k_fold_mod <- function(data, target_col, model_formula, k = 10) {
  
  set.seed(123)
  folds <- createFolds(data[[target_col]], k = k, list = TRUE, returnTrain = FALSE)
  
  acc_best_vec     <- numeric(k)
  f1_best_vec      <- numeric(k)
  auc_vec          <- numeric(k)
  aic_vec          <- numeric(k)
  acc_thresh_vec   <- numeric(k)
  f1_thresh_vec    <- numeric(k)
  sensitivity_vec  <- numeric(k)
  specificity_vec  <- numeric(k)
  
  for (i in 1:k) {
    test_idx <- folds[[i]]
    train_fold <- data[-test_idx, ]
    test_fold  <- data[test_idx, ]
    
    # Fit model
    fitted_model <- glm(model_formula, data = train_fold, family = binomial)
    pred_probs_train <- predict(fitted_model, newdata = train_fold, type = "response")
    actual_train <- train_fold[[target_col]]
    
    # Threshold search
    thresholds <- seq(0.01, 0.99, by = 0.01)
    acc_scores <- numeric(length(thresholds))
    f1_scores  <- numeric(length(thresholds))
    
    for (j in seq_along(thresholds)) {
      threshold <- thresholds[j]
      preds <- ifelse(pred_probs_train > threshold, 1, 0)
      acc_scores[j] <- mean(preds == actual_train)
      
      cm <- table(Predicted = preds, Actual = actual_train)
      precision <- ifelse("1" %in% rownames(cm) && "1" %in% colnames(cm),
                          cm["1", "1"] / sum(cm["1", ]), 0)
      recall <- ifelse("1" %in% rownames(cm) && "1" %in% colnames(cm),
                       cm["1", "1"] / sum(cm[, "1"]), 0)
      f1 <- ifelse((precision + recall) > 0,
                   2 * (precision * recall) / (precision + recall), 0)
      f1_scores[j] <- f1
    }
    
    # Best thresholds
    best_acc_threshold <- thresholds[which.max(acc_scores)]
    best_f1_threshold  <- thresholds[which.max(f1_scores)]
    acc_thresh_vec[i] <- best_acc_threshold
    f1_thresh_vec[i]  <- best_f1_threshold
    
    # Test predictions
    pred_probs_test <- predict(fitted_model, newdata = test_fold, type = "response")
    actual_test <- test_fold[[target_col]]
    
    pred_acc <- ifelse(pred_probs_test > best_acc_threshold, 1, 0)
    pred_f1  <- ifelse(pred_probs_test > best_f1_threshold, 1, 0)
    acc_best_vec[i] <- mean(pred_acc == actual_test)
    
    # Confusion matrix for F1 threshold
    cm <- table(Predicted = pred_f1, Actual = actual_test)
    
    tp <- ifelse("1" %in% rownames(cm) && "1" %in% colnames(cm), cm["1", "1"], 0)
    tn <- ifelse("0" %in% rownames(cm) && "0" %in% colnames(cm), cm["0", "0"], 0)
    fp <- ifelse("1" %in% rownames(cm) && "0" %in% colnames(cm), cm["1", "0"], 0)
    fn <- ifelse("0" %in% rownames(cm) && "1" %in% colnames(cm), cm["0", "1"], 0)
    
    precision <- ifelse((tp + fp) > 0, tp / (tp + fp), 0)
    recall    <- ifelse((tp + fn) > 0, tp / (tp + fn), 0)
    
    f1_best_vec[i] <- ifelse((precision + recall) > 0,
                             2 * (precision * recall) / (precision + recall), 0)
    
    sensitivity_vec[i] <- ifelse((tp + fn) > 0, tp / (tp + fn), NA)
    specificity_vec[i] <- ifelse((tn + fp) > 0, tn / (tn + fp), NA)
    
    auc_vec[i] <- tryCatch({
      roc_obj <- roc(actual_test, pred_probs_test)
      as.numeric(auc(roc_obj))
    }, error = function(e) NA)
    
    aic_vec[i] <- AIC(fitted_model)
  }
  
  return(list(
    Accuracy_at_best_threshold = paste0(round(mean(acc_best_vec, na.rm = TRUE), 4),
                                        " (threshold = ", round(mean(acc_thresh_vec, na.rm = TRUE), 2), ")"),
    F1_at_best_threshold       = paste0(round(mean(f1_best_vec, na.rm = TRUE), 4),
                                        " (threshold = ", round(mean(f1_thresh_vec, na.rm = TRUE), 2), ")"),
    Sensitivity                = round(mean(sensitivity_vec, na.rm = TRUE), 4),
    Specificity                = round(mean(specificity_vec, na.rm = TRUE), 4),
    AUC                        = round(mean(auc_vec, na.rm = TRUE), 4),
    AIC                        = round(mean(aic_vec, na.rm = TRUE), 2)
  ))
}

evaluate_threshold <- function(probs, target, threshold) {
  pred <- ifelse(probs > threshold, 1, 0)
  cm <- table(Predicted = pred, Actual = target)

  tp <- ifelse("1" %in% rownames(cm) && "1" %in% colnames(cm), cm["1", "1"], 0)
  tn <- ifelse("0" %in% rownames(cm) && "0" %in% colnames(cm), cm["0", "0"], 0)
  fp <- ifelse("1" %in% rownames(cm) && "0" %in% colnames(cm), cm["1", "0"], 0)
  fn <- ifelse("0" %in% rownames(cm) && "1" %in% colnames(cm), cm["0", "1"], 0)

  accuracy    <- (tp + tn) / (tp + tn + fp + fn)
  precision   <- ifelse((tp + fp) > 0, tp / (tp + fp), 0)
  recall      <- ifelse((tp + fn) > 0, tp / (tp + fn), 0)
  specificity <- ifelse((tn + fp) > 0, tn / (tn + fp), 0)
  f1          <- ifelse((precision + recall) > 0,
                        2 * (precision * recall) / (precision + recall), 0)

  return(list(
    Threshold   = threshold,
    Accuracy    = round(accuracy, 4),
    F1          = round(f1, 4),
    Sensitivity = round(recall, 4),
    Specificity = round(specificity, 4)
  ))
}

```

### Comperison
```{r}
summary(stepwise)
summary(lasso_mod)

# Compare the models
stepwise_results <- k_fold_mod(data = full_df, target_col = "target", model_formula = stepwise)
lasso_results    <- k_fold_mod(data = full_df, target_col = "target", model_formula = lasso_mod)

print(stepwise_results)
print(lasso_results)

# Threshold evaluation
probs_stepwise <- predict(stepwise, type = "response")
probs_lasso    <- predict(lasso_mod, type = "response")

res_step_05 <- evaluate_threshold(probs_stepwise, target, 0.5)
res_step_02 <- evaluate_threshold(probs_stepwise, target, 0.2)

res_lasso_05 <- evaluate_threshold(probs_lasso, target, 0.5)
res_lasso_02 <- evaluate_threshold(probs_lasso, target, 0.2)


# Unisci tutti i risultati in una lista
results_list <- list(
  Stepwise_0.5 = res_step_05,
  Stepwise_0.2 = res_step_02,
  LASSO_0.5    = res_lasso_05,
  LASSO_0.2    = res_lasso_02
)

# Trasforma in data.frame
results_df <- do.call(rbind, lapply(names(results_list), function(name) {
  res <- results_list[[name]]
  model <- sub("_.*", "", name)
  threshold <- res$Threshold
  data.frame(
    Model       = model,
    Threshold   = threshold,
    Accuracy    = res$Accuracy,
    F1          = res$F1,
    Sensitivity = res$Sensitivity,
    Specificity = res$Specificity
  )
}))

# Visualizza il risultato
print(results_df)

```

