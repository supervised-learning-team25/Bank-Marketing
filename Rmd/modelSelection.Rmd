## Model selection

```{r, echo=FALSE}
source("../R/MS_functions.R")
```


### Preprocessing
Based on the Exploratory Data Analysis (EDA), we selected only the most relevant variables.

With a view to training the model, we apply **one-hot encoding**. We obtain the following dataset:

```{r, echo=FALSE}
train_bin <- train

train_bin$cons.price.idx <- ifelse(train_bin$cons.price.idx < 93, 1, 0)
names(train_bin)[names(train_bin) == "cons.price.idx"] <- "low_cpi"

train_bin$cons.conf.idx <- ifelse(train_bin$cons.conf.idx > median(train$cons.conf.idx), 1, 0)
names(train_bin)[names(train_bin) == "cons.conf.idx"] <- "high_cci"

train_bin$euribor3m <- ifelse(train_bin$euribor3m < mean(train$euribor3m), 1, 0)
names(train_bin)[names(train_bin) == "euribor3m"] <- "low_euribor"

train_bin$emp.var.rate <- ifelse(train_bin$emp.var.rate < 0, 1, 0)
names(train_bin)[names(train_bin) == "emp.var.rate"] <- "negative_emp"

train_bin$university<-ifelse(train_bin$education=='university.degree', 1, 0)
train_bin$p_course<-ifelse(train_bin$education=='professional.course', 1, 0)
train_bin <- subset(train_bin, select = -education)

train_bin$job_student <- ifelse(train_bin$job == "student", 1, 0)
train_bin$job_retired <- ifelse(train_bin$job == "retired", 1, 0)
train_bin$job_admin <- ifelse(train_bin$job == "admin.", 1, 0)
train_bin <- subset(train_bin, select = -job)

train_bin$month_sep <- ifelse(train_bin$month == "sep", 1, 0)
train_bin$month_oct <- ifelse(train_bin$month == "oct", 1, 0)
train_bin$month_dec <- ifelse(train_bin$month == "dec", 1, 0)
train_bin$month_mar <- ifelse(train_bin$month == "mar", 1, 0)

train_bin$p_failure <- ifelse(train_bin$poutcome == "failure", 1, 0)
train_bin$p_success <- ifelse(train_bin$poutcome == "success", 1, 0)
train_bin <- subset(train_bin, select = -poutcome)

train_bin$contact <- ifelse(train_bin$contact == "cellular", 1, 0)
names(train_bin)[names(train_bin) == "contact"] <- "cellular"

train_bin$marital <- ifelse(train_bin$marital == "single", 1, 0)
names(train_bin)[names(train_bin) == "marital"] <- "single"

train_bin$campaign <- ifelse(train_bin$campaign > 5, 0, 1)
names(train_bin)[names(train_bin) == "campaign"] <- "low_call"

```

```{r, echo=FALSE}
drop_var<-c('pdays', "default", 'housing' ,"loan", "day_of_week", 
            "month", "nr.employed", 'job', "duration")

full_df <- train_bin[, !(names(train_bin) %in% drop_var)]
full_df$target <-factor(ifelse(subscribed=='yes', 1, 0))
full_df<-full_df[, !(names(full_df)) %in% 'subscribed']
detach(train)
attach(full_df)
```

```{r, echo=FALSE}
varDf <- data.frame(
  Variable = c("age", "single", "cellular", "low_call", "previous", "negative_emp", "low_cpi", "high_cci", "low_euribor", "university", "p_course", "job_student", "job_retired", "job_admin", "month_sep", "month_oct", "month_dec", "month_mar", "p_failure", "p_success", "target"),
  Type = c("int", "bool", "bool", "bool", "int", "bool", "bool", "bool", "bool", "bool", "bool", "bool", "bool", "bool", "bool", "bool", "bool", "bool", "bool", "bool", "bool")
)

knitr::kable(varDf, align = "ll", caption = "21 Variable") %>%
kableExtra::kable_styling(full_width = TRUE, position = "center", bootstrap_options = "striped")
```

### Logistic Regression

**Logistic regression** is a widely used statistical model for binary **classification** tasks. It is based on the sigmoid (logistic) function, which maps any real-valued input into the interval (0, 1), making the output interpretable as a **probability**. Specifically, the probability that an observation belongs to class 1 is given by:

<div class="center">
$P(Y=1|X=x)=p(x)=\frac{e^{\beta_0+\beta_1x_1+\dots + \beta_n  x_n}}{1+e^{\beta_0+\beta_1x_1+\dots + \beta_n x_n}}$
</div>

To perform classification, a **decision threshold** is applied: if $p(x)$ exceeds the threshold (commonly 0.5), the observation is assigned to class 1; otherwise, it is assigned to class 0.

Before training the model on the training set, we applied **two variable selection methods** to the entire dataset in order to reduce the large number of predictors. The selection criteria were: 

- **Stepwise** selection in both directions, that optimizes the AIC at each step.

- **LASSO,** which, as supported by the literature, often outperforms stepwise methods. 

After that we checked for VIF, but for both models **there wasn’t any multicollinearity.**

Once the relevant variables were selected using both methods, we employed **k-fold cross-validation** to obtain stable estimates of accuracy, misclassification error, and other useful metrics which will be used for comparison. We chose 10 folds, as it offers a good balance between computational efficiency, compared to LOOCV, and reliable model evaluation, as compared to 5-fold cross-validation.

The variables we got from stepwise and LASSO are the following ones:

```{r, echo= FALSE}
df <- data.frame(
  Full_dataset = c('age', 'single', 'cellular', 'low_call', 'previous', 'negative_emp', 'low_cpi', 'high_cci', 'low_euribor', 'university', 'p_course', 'job_student', 'job_retired', 'job_admin', 'month_sep', 'month_oct', 'month_dec', 'month_mar', 'p_failure', 'p_success', 'total'),
  Stepwise_Model = c(' ', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', '20'),
  Lasso_Model=c(' ', ' ', 'x', ' ', ' ', 'x', 'x', 'x', 'x', 'x', ' ', 'x', 'x', ' ', 'x', 'x', 'x', 'x', 'x', 'x', '14')
)

knitr::kable(df, align = 'ccc', caption = 'Model Variable Comparison')%>%
  kableExtra::kable_styling(full_width = TRUE, position='center', bootstrap_options = c("striped"))

```

As we can see, **LASSO** was able to **shrink the number of variables more than Stepwise**. It's a simpler model. Let's now compare the two models at different thresholds. The **thresholds** maximize: 

- **Accuracy,** that takes into account just the general accuracy of the model.

- **F1** which is a metric useful in unbalanced dataset like ours, since it considers both precision (how many predicted positives are actual positives) and recall (how many actual positives are detected).

We also have computed Sensitivity and Specificity.

```{r echo=FALSE, message=FALSE, warning=FALSE}
full_model <- glm(target ~ ., data = full_df, family = binomial)
stepwise <- stepAIC(full_model, direction = "both", trace = FALSE)

# predictore removed by Stepwise
stepwise_formula <- target ~ single + cellular + low_call + previous + negative_emp + 
  low_cpi + high_cci + low_euribor + university + p_course + 
  job_student + job_retired + job_admin + month_sep + month_oct + 
  month_dec + month_mar + p_failure + p_success



set.seed(123)

df_no_target <- subset(full_df, select = -target)

fit_lasso <- glmnet(x = as.matrix(df_no_target),
                    y = target,
                    alpha = 1,
                    family = "binomial",
)

cv_fit <- cv.glmnet(
  x = as.matrix(df_no_target),
  y = target,
  alpha = 1,
  family = "binomial"
)

# predictors selected by Lasso

lasso_formula <- target ~ cellular + negative_emp + 
  low_cpi + high_cci + low_euribor + university + 
  job_student + job_retired + month_sep + month_oct + 
  month_dec + month_mar + p_failure + p_success

lasso_mod<-glm(lasso_formula, data=full_df, family=binomial)
```


```{r echo=FALSE, message=FALSE}
# Compare the models
stepwise_results <- k_fold_mod(data = full_df, target_col = "target", model_formula = stepwise)
lasso_results    <- k_fold_mod(data = full_df, target_col = "target", model_formula = lasso_mod)

# Threshold evaluation
probs_stepwise <- predict(stepwise, type = "response")
probs_lasso    <- predict(lasso_mod, type = "response")

res_step_05 <- evaluate_threshold(probs_stepwise, target, 0.5)
res_step_02 <- evaluate_threshold(probs_stepwise, target, 0.2)

res_lasso_05 <- evaluate_threshold(probs_lasso, target, 0.5)
res_lasso_02 <- evaluate_threshold(probs_lasso, target, 0.2)


# Unisci tutti i risultati in una lista
results_list <- list(
  Stepwise_0.5 = res_step_05,
  Stepwise_0.2 = res_step_02,
  LASSO_0.5    = res_lasso_05,
  LASSO_0.2    = res_lasso_02
)

# Trasforma in data.frame
results_df <- do.call(rbind, lapply(names(results_list), function(name) {
  res <- results_list[[name]]
  model <- sub("_.*", "", name)
  threshold <- res$Threshold
  data.frame(
    Model       = model,
    Threshold   = threshold,
    Accuracy    = res$Accuracy,
    F1          = res$F1,
    Sensitivity = res$Sensitivity,
    Specificity = res$Specificity
  )
}))

# Visualizza il risultato

knitr::kable(results_df, align = 'ccccc', caption = 'Model Performance at Different Thresholds')  %>%
  kableExtra::kable_styling(full_width = TRUE, position='center', bootstrap_options = c("striped"))

```
The table shows that both models perform similarly in terms of accuracy and specificity. However, when the threshold is lowered from 0.5 to 0.2, both models achieve higher F1 scores and sensitivity, though at the expense of reduced accuracy and specificity.

Between the two models, the LASSO-based logistic regression is slightly preferable, as it achieves comparable or slightly better F1 scores while benefiting from variable selection and model simplicity.

Given the imbalance in the dataset, we prioritize a threshold that maximizes the F1 score rather than overall accuracy. This is because both false positives (wasting resources contacting uninterested customers) and false negatives (missing likely subscribers) are costly in this context. Therefore, a threshold that better balances precision and recall, reflected by a higher F1 score, is more appropriate for the bank’s decision-making.

### LDA

#### Models
```{r, echo=FALSE}
lda_full_model <- lda(target ~ ., data = full_df)

lda_stepwise_model <- lda(stepwise_formula, data = full_df)

lda_lasso_model <- lda(lasso_formula, data = full_df)
```

#### CV
```{r}
results_lda_full <- cv_lda_eval(full_df, target ~ ., model_name = "LDA_Full", k = 10)

results_lda_stepwise <- cv_lda_eval(full_df,stepwise_formula, model_name = "LDA_Stepwise", k = 10)

results_lda_lasso <- cv_lda_eval(full_df, lasso_formula, model_name = "LDA_LASSO", k = 10)
```

#### Evaluate

```{r}
lda_results <- rbind(results_lda_full, results_lda_stepwise, results_lda_lasso)

aggregate(cbind(Accuracy, F1, Sensitivity, Specificity) ~ Model + Optimized_For, data = lda_results, mean)
aggregate(Threshold ~ Model + Optimized_For, data = lda_results, mean)
```


#### Best LDA

```{r}
print(lda_lasso_model)

probs <- predict(lda_lasso_model)$posterior[, "1"]
pred_class <- ifelse(probs > 0.16, 1, 0)
table(Predicted = pred_class, Actual = full_df$target)

evaluate_threshold(probs, full_df$target, threshold = 0.16)
```

#### ROC Curve

```{r}
roc_obj <- roc(full_df$target, probs)
plot(roc_obj, main = "ROC Curve for LDA LASSO Model")
auc(roc_obj)
```


### QDA

#### Models
```{r, echo=FALSE}
#qda_full_model     <- qda(target ~ ., data = full_df)
#qda_stepwise_model <- qda(stepwise_formula, data = full_df)
#qda_lasso_model    <- qda(lasso_formula, data = full_df)
```

#### CV
```{r}
#results_qda_full     <- cv_qda_eval(full_df, target ~ ., model_name = "QDA_Full", k = 10)
#results_qda_stepwise <- cv_qda_eval(full_df, stepwise_formula, model_name = "QDA_Stepwise", k = 10)
#results_qda_lasso    <- cv_qda_eval(full_df, lasso_formula, model_name = "QDA_LASSO", k = 10)
```

#### Evaluate
```{r}
#qda_results <- rbind(results_qda_full, results_qda_stepwise, results_qda_lasso)

#aggregate(cbind(Accuracy, F1, Sensitivity, Specificity) ~ Model + Optimized_For, data = qda_results, mean)
#aggregate(Threshold ~ Model + Optimized_For, data = qda_results, mean)
```

#### Best QDA
```{r}
#print(qda_lasso_model)

#probs_qda <- predict(qda_lasso_model)$posterior[, "1"]
#pred_class_qda <- ifelse(probs_qda > 0.4, 1, 0)
#table(Predicted = pred_class_qda, Actual = full_df$target)

#evaluate_threshold(probs_qda, full_df$target, threshold = 0.4)
```

#### ROC Curve
```{r}
#roc_qda <- roc(full_df$target, probs_qda)
#plot(roc_qda, main = "ROC Curve for QDA LASSO Model")
#auc(roc_qda)
```


### Random Forest

```{r}
df_for_Tree <- full_df
df_for_Tree$target <- factor(ifelse(full_df$target == 1, "Yes", "No"))

train_index <- createDataPartition(df_for_Tree$target, p = 0.8, list = FALSE)

train_set <- df_for_Tree[train_index, ]
test_set  <- df_for_Tree[-train_index, ]
```

```{r}
set.seed(123)
treeDf <- tree(target ~ ., train_set)

summary(treeDf)
```

```{r}
plot(treeDf)
text(treeDf, pretty = 0)
```

```{r}
#cvDf <- cv.tree(treeDf)
#cvDf
#plot(cvDf$size, cvDf$dev , type = "b")
```


```{r}
set.seed(123)
rf_train <- randomForest(target ~ ., train_set, mtry = 7, importance = TRUE)
rf_train
```

```{r}
tree_pred <- predict(rf_train, newdata = test_set)
table(tree_pred, test_set$target)
```

```{r}
importance(rf_train)
```

```{r}
varImpPlot(rf_train)
```

```{r}
#x <- full_df[, !(names(df_for_Tree) %in% "target")]
#y <- full_df$target

#xtrain <- x[train_index, ]
#ytrain <- y[train_index]

#xtest <- x[-train_index, ]
#ytest <- y[-train_index]

#set.seed(123)
#bartfit <- lbart(xtrain, ytrain, x.test = xtest)
```

```{r}
#yhat_bart <- bartfit$prob.test.mean
#table(yhat_bart, test_set$target)
#table(pred = yhat_bart > 0.5, actual = ytest)
```